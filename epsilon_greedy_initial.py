# -*- coding: utf-8 -*-
"""Epsilon Greedy_initial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxO_mxndb31KUFiZrCws5pFslAUV4g4g
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

num_of_exp=10000
eps=0.1
bandith_true_probab=[0.2,0.5,0.75] ### generallly these are not provided we have used this to generate a kind of simulation of the original environment

class BanditArm: ## this is just a class with two functions pull and update: pull is to run that bandith and update is to update the trial
    def __init__(self,p):
        self.p=p  ### this is the wining probability for the current bandith in place
        self.p_estimate=0 ### this is the estimated probability for the current bandith wining rate
        self.N=0 ## this is the trial number i.e what is the trial that is happening
    def pull(self):
        return np.random.random()<self.p  ### this is just a way to provide reward, currently this is based on the binomial 0/1 but it can be real number too
    def update(self,x):
        self.N=self.N+1 ### so thi N an Num_of_exp are different this n is for each bandit how many trials we ave performed
        self.p_estimate=((self.N-1)*self.p_estimate+x)/self.N  ### this is the way by which we updte the p_estimates of the current bandith, toh yeh probbaility eastimate hai win rate ka

def experiment(): ### how you design a banditarm epsilon greedy experiment
    bandit=[BanditArm(p) for p in bandith_true_probab] ### initializing each arm via giving og win rate for each bandit
    rewards=np.zeros(num_of_exp) ## initializing the empty list for rewards
    ### few book keeping variables
    num_of_explore=0
    num_of_explot=0
    num_of_opt=0
    opt_j =np.argmax([b.p for b in bandit])
    ### running the experiments and getting rewards and all
    for i in range(num_of_exp):
        if np.random.random()<eps:
            num_of_explore+=1
            j=np.random.randint(len(bandit))
        else:
            j=opt_j
            num_of_explot+=1

        if j==opt_j:
            num_of_opt+=1

        x=bandit[j].pull()
        rewards[i]=x
        bandit[j].update(x)

    ### getting few prints:
    for b in bandit:
        print("estimated_win_rates: ",b.p_estimate)
    ### for priniting some info:
    print("total reward", rewards.sum())
    print("overall_win_rate", rewards.sum()/num_of_exp)
    print("number of times_explore: ", num_of_explore)
    print("number of times_exploit: ", num_of_explot)
    print("number of time opt j is selected: ", num_of_opt)

    ### for plotting
    cummul_win_rate=np.cumsum(rewards)
    win_rate=cummul_win_rate/np.arange(1,num_of_exp+1)
    plt.plot(win_rate)
    plt.show()

experiment()

